class MyAIUtil2 {
  use Fn;
  use Math;
  use List;
  use Hash;
  use IntList;
  use Array;
  use Format;
  use Sys;
  use R::OP::Int as IOP;
  use R::OP::Float as FOP;
  use R::OP::Matrix::Float as MFOP;
  use R::NDArray::Float;
  
  # Total executions
  our $TOTAL_COUNT : int;

  # Number of correct answers
  our $ANSWER_MATCH_COUNT : int;
  
  # seed
  our $SEED : int;
  
  INIT {
    $SEED = (int)Sys->time;
  }
  
  
  # Shuffle
  static method shufflei : int[] ($array_original : int[]) {
    my $size = @$array_original;
    my $array = Array->copy_int($array_original);
    for (my $i = 0; $i < $size; $i++) {
      my $seed = $SEED;
      my $j = Fn->crand(\$seed) % $size;
      $SEED = $seed;
      my $t = $array->[$i];
      $array->[$i] = $array->[$j];
      $array->[$j] = $t;
    }
    return $array;
  }
  
  static method train_deep_network : void ($mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash,
    $epoch_count : int, $mini_batch_size : int, $neurons_count_in_layers : int[], $learning_rate : int)
  {
    # Information on the function that converts m inputs to n outputs for each layer. Number of inputs, number of outputs, bias, weights
    my $m_to_n_func_infos = &init_m_to_n_func_infos($neurons_count_in_layers);

    # Training data index (only the first 40,000 images are used as training data. The remaining 10,000 images are used as validation data)
    my $training_data_indexes = new int[40000];
    my $cur_training_data_index = 0;
    for (my $i = 0; $i < @$training_data_indexes; $i++) {
      $training_data_indexes->[$i] = $i;
    }

    # Information on each transformation function in a mini-batch
    my $m_to_n_func_mini_batch_infos = &init_m_to_n_func_mini_batch_infos($m_to_n_func_infos);

    # Run the training set for a number of epochs
    for (my $epoch_index = 0; $epoch_index < $epoch_count; $epoch_index++) {
      
      # Shuffle the training data index (it seems that random learning is more generalizable)
      my $training_data_indexes_shuffle = &shufflei($training_data_indexes);
      
      # Update parameters using stochastic gradient descent
      &update_params_sgd(
        $m_to_n_func_mini_batch_infos,
        $m_to_n_func_infos,
        $training_data_indexes_shuffle,
        $mini_batch_size,
        $mnist_train_image_info_spvm,
        $mnist_train_label_info_spvm,
        $learning_rate
      );
    }
  }
  
  static method update_params_sgd : void ($m_to_n_func_mini_batch_infos : List, $m_to_n_func_infos : List, $training_data_indexes_shuffle : int[],
    $mini_batch_size: int, $mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash, $learning_rate : float) {
 
    for (my $training_data_indexes_shuffle_index = 0; $training_data_indexes_shuffle_index < @$training_data_indexes_shuffle; $training_data_indexes_shuffle_index += $mini_batch_size) {
      
      # Initialize the sum of the bias slopes of each transformation function in the mini-batch and the sum of the weight slopes of each transformation function in the mini-batch to 0.
      for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_mini_batch_infos->length; $m_to_n_func_index++) {
        # The sum of the bias slopes of each transformation function in the mini-batch is initialized to 0.
        my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
        
        my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
        $m_to_n_func_mini_batch_info->set(biase_grad_totals => FOP->c(undef, [$biase_grad_totals->length]));
        
        # The sum of the weight gradients of each transformation function in the mini-batch is initialized to 0.
        my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
        
        # $weight_grad_totals_mat->set_data(FOP->c(undef, [$weight_grad_totals_mat->length]);
        
        Fn->memcpy($weight_grad_totals_mat->data, 0, FOP->c(undef, [$weight_grad_totals_mat->length])->data, 0, 4 * $weight_grad_totals_mat->length);
      }
      
      for (my $mini_batch_index = 0; $mini_batch_index < $mini_batch_size; $mini_batch_index++) {        
        my $training_data_index = $training_data_indexes_shuffle->[$training_data_indexes_shuffle_index + $mini_batch_index];
        
        # Use backpropagation to get the gradient of the weights and biases with respect to the loss function
        my $m_to_n_func_grad_infos = &backprop($m_to_n_func_infos, $mnist_train_image_info_spvm, $mnist_train_label_info_spvm, $training_data_index);
        
        # The slope of the bias loss function
        my $biase_grads_list = (List)$m_to_n_func_grad_infos->get("biases");
        
        # The gradient of the weights with respect to the loss function
        my $weight_grads_mat_list = (List)$m_to_n_func_grad_infos->get("weights_mat");

        # Add the sum of the bias slopes of each transformation function in the mini-batch and the weight slopes of each transformation function in the mini-batch
        for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_mini_batch_infos->length; $m_to_n_func_index++) {
          my $m_to_n_func_info = $m_to_n_func_infos->get($m_to_n_func_index);
          
          # Add the bias slope of each transformation function in the mini-batch
          my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
          my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
          my $biase_grads = (R::NDArray::Float)$biase_grads_list->get($m_to_n_func_index);
          $biase_grad_totals = FOP->add($biase_grad_totals, $biase_grads);

          # Add the gradient of the weights of each transformation function in the mini-batch
          my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
          my $weight_grads_mat = (R::NDArray::Float)$weight_grads_mat_list->get($m_to_n_func_index);
          
          # $weight_grad_totals_mat->{data} = FOP->add($weight_grad_totals_mat, $weight_grads_mat)->data;
          
          Fn->memcpy($weight_grad_totals_mat->data, 0, FOP->add($weight_grad_totals_mat, $weight_grads_mat)->data, 0, 4 * @{$weight_grad_totals_mat->data});
        }
      }

      # Update the biases and weights of each transformation function using the sum of the gradients over the mini-batch
      for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
        
        # Update the biases of each transformation function (taking into account the learning rate and dividing by the number of mini-batches)
        my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
        my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
        
        my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
        my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
        &update_params($biases, $biase_grad_totals, $learning_rate, $mini_batch_size);
        
        # Update the weights of each transformation function (taking into account the learning rate, sum of the gradients by the number of mini-batches, divided by the number of mini-batches)
        my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
        my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
        &update_params($weights_mat, $weight_grad_totals_mat, $learning_rate, $mini_batch_size);
      }
    }
  }

  static method init_m_to_n_func_infos : List ($neurons_count_in_layers : int[]) {
    # Information on the function that converts m inputs to n outputs for each layer. Number of inputs, number of outputs, bias, weights
    my $m_to_n_func_infos = List->new([]);
    
    # Create information on the conversion function from m to n based on the number of neurons in each layer
    for (my $i = 0; $i < @$neurons_count_in_layers - 1; $i++) {
      my $inputs_length = $neurons_count_in_layers->[$i];
      my $outputs_length = $neurons_count_in_layers->[$i + 1];
      
      # Initialize bias to 0
      my $biases = FOP->c(undef, [$outputs_length]);
      
      # Initialize weights with Xavier initial values. Weights are column-major matrix.
      my $weights_length = $outputs_length * $inputs_length;
      my $weights_mat = MFOP->matrix(&array_create_xavier_init_value($weights_length, $inputs_length), $outputs_length, $inputs_length);
      
      # Set conversion function information
      my $m_to_n_func_info = Hash->new({});
      $m_to_n_func_info->set_int(inputs_length => $inputs_length);
      $m_to_n_func_info->set_int(outputs_length => $outputs_length);
      $m_to_n_func_info->set(biases => $biases);
      $m_to_n_func_info->set(weights_mat => $weights_mat);
      
      $m_to_n_func_infos->push($m_to_n_func_info);
    }
    
    return $m_to_n_func_infos;
  }

  static method init_m_to_n_func_mini_batch_infos : List ($m_to_n_func_infos : List) {
    # Information on each transformation function in a mini-batch
    
    my $m_to_n_func_mini_batch_infos = List->new_len(new object[0], $m_to_n_func_infos->length);

    # The sum of the bias slopes of each transformation function in the mini-batch and the sum of the weight slopes of each transformation function in the mini-batch are initialized to 0.
    # Initialize the memory area here to use it repeatedly
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
      my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
      
      # Bias Length
      my $biases_length = $biases->length;

      # The sum of the bias slopes of each transformation function in the mini-batch is initialized to 0.
      my $biase_grad_totals = FOP->c(undef, [$biases_length]);

      # The sum of the weight gradients of each transformation function in the mini-batch is initialized to 0.
      my $weight_grad_totals_mat = MFOP->matrix(undef, $weights_mat->nrow, $weights_mat->ncol);
      
      my $m_to_n_func_mini_batch_info = Hash->new({});
      $m_to_n_func_mini_batch_info->set(biase_grad_totals => $biase_grad_totals);
      $m_to_n_func_mini_batch_info->set(weight_grad_totals_mat => $weight_grad_totals_mat);
      
      $m_to_n_func_mini_batch_infos->set($m_to_n_func_index => $m_to_n_func_mini_batch_info);
    }
    
    return $m_to_n_func_mini_batch_infos;
  }

  # Backpropagation
  static method backprop : Hash ($m_to_n_func_infos : List, $mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash, $training_data_index : int)  {
    
    my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get(0);
    my $first_inputs_length = (int)$m_to_n_func_info->get("inputs_length");
    
    # Input (converts values ​​from 0 to 255 into 0 to 1)
    my $mnist_train_image_rows_count = (int)$mnist_train_image_info_spvm->get("rows_count");
    my $mnist_train_image_columns_count = (int)$mnist_train_image_info_spvm->get("columns_count");
    my $image_unit_length = $mnist_train_image_rows_count * $mnist_train_image_columns_count;
    my $mnist_train_image_data = (byte[])$mnist_train_image_info_spvm->get("data");
    
    my $first_inputs_raw_uint8 = Array->copy_byte($mnist_train_image_data, $image_unit_length * $training_data_index, $image_unit_length);
    my $first_inputs_raw_float = FOP->c(&convert_ubyte_array_to_float_array($first_inputs_raw_uint8));
    
    my $first_inputs = FOP->scadiv($first_inputs_raw_float, FOP->c(255f));
    
    # Generate a probability distribution for the expected outputる
    my $label_numbers = (IntList)$mnist_train_label_info_spvm->get("label_numbers");
    my $label_number = (Int)$label_numbers->get($training_data_index);
    my $desired_outputs = &probabilize_desired_outputs($label_number);
    
    # The bias slope of each transformation function
    my $biase_grads_in_m_to_n_funcs = List->new_len(new R::NDArray::Float[0], $m_to_n_func_infos->length);
    
    # The weight gradient of each transformation function
    my $weight_grads_mat_in_m_to_n_funcs = List->new_len(new R::NDArray::Float[0], $m_to_n_func_infos->length);
    
    # Initialization of bias gradients and weight gradients
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      my $inputs_length = (int)$m_to_n_func_info->get("inputs_length");
      my $outputs_length = (int)$m_to_n_func_info->get("outputs_length");
      
      # Initialize the bias slope to 0
      $biase_grads_in_m_to_n_funcs->set($m_to_n_func_index => FOP->c(undef, [$outputs_length]));

      # Initialize weight gradients to 0
      $weight_grads_mat_in_m_to_n_funcs->set($m_to_n_func_index => MFOP->matrix(undef, $outputs_length, $inputs_length));
    }

    # Input for each layer
    my $inputs_in_m_to_n_funcs = List->new(new R::NDArray::Float[0]);
    $inputs_in_m_to_n_funcs->push($first_inputs);
    
    
    # Activation output of each layer
    my $outputs_in_m_to_n_funcs = List->new(new R::NDArray::Float[0]);
    
    # Obtain the output of the output layer from the input of the input layer
    # Save each layer's output and activations for backpropagation
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $cur_inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($inputs_in_m_to_n_funcs->length - 1);
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      
      # Weight matrix
      my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
      
      # Input Matrix
      my $cur_inputs_nrow = $cur_inputs->length;
      my $cur_inputs_ncol = 1;
      
      my $cur_inputs_mat = MFOP->matrix($cur_inputs, $cur_inputs_nrow, $cur_inputs_ncol);
      
      # Matrix multiplication of weights and inputs
      my $mul_weights_inputs_mat = MFOP->mul($weights_mat, $cur_inputs_mat);
      $mul_weights_inputs_mat->drop_dim;
      my $mul_weights_inputs = $mul_weights_inputs_mat;
      
      # bias
      my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
      
      # Output - The sum of the weights and inputs matrix products and the biases
      my $outputs = FOP->add($mul_weights_inputs, $biases);
      
      # Activated output - Apply the activation function to the output
      my $activate_outputs = &array_sigmoid($outputs);
      
      # Save output for backpropagation
      $outputs_in_m_to_n_funcs->push($outputs);
      
      # Save next input for backpropagation
      $inputs_in_m_to_n_funcs->push($activate_outputs);
    }
    
    # Final output
    my $last_outputs = (R::NDArray::Float)$outputs_in_m_to_n_funcs->get($outputs_in_m_to_n_funcs->length - 1);
    
    # Last Activated Output
    my $last_activate_outputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->pop;
    
    # error
    
    my $cost = &cross_entropy_cost($last_activate_outputs, $desired_outputs)->data->[0];
    print "Cost: " . Format->sprintf("%.3f", [(object)$cost]) . "\n";
    
    # Did you get it right?
    my $answer = &max_index($last_activate_outputs)->data->[0];
    my $desired_answer = &max_index($desired_outputs)->data->[0];
    $TOTAL_COUNT++;
    if ($answer == $desired_answer) {
      $ANSWER_MATCH_COUNT++;
    }
    
    # Output accuracy rate
    my $match_rate = (double)$ANSWER_MATCH_COUNT / $TOTAL_COUNT;
    print "Match Rate: " . Format->sprintf("%.02f", [(object)(100 * $match_rate)]) . "%\n";
    
    # Activated Output Change / Last Output Change
    my $grad_last_outputs_to_activate_func = &array_sigmoid_derivative($last_outputs);
    
    # Small change in loss function / Small change in last activated output
    my $grad_last_activate_outputs_to_cost_func = &cross_entropy_cost_derivative($last_activate_outputs, $desired_outputs);

    # Small change in loss function / Small change in final output (composite derivative)
    my $grad_last_outputs_to_cost_func = FOP->mul($grad_last_outputs_to_activate_func, $grad_last_activate_outputs_to_cost_func);

    # Small changes in loss function / small changes in bias of final layer
    my $last_biase_grads = $grad_last_outputs_to_cost_func;
    

    # Small changes in loss function / small changes in weights of the final layer
    my $last_biase_grads_mat = MFOP->matrix($last_biase_grads, $last_biase_grads->length, 1);
    my $last_inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($inputs_in_m_to_n_funcs->length - 1);
    my $last_inputs_transpose_mat = MFOP->matrix($last_inputs, 1, $last_inputs->length);
    my $last_weight_grads_mat = MFOP->mul($last_biase_grads_mat, $last_inputs_transpose_mat);
      
    $biase_grads_in_m_to_n_funcs->set($biase_grads_in_m_to_n_funcs->length - 1 => $last_biase_grads);
    $weight_grads_mat_in_m_to_n_funcs->set($biase_grads_in_m_to_n_funcs->length - 1 => $last_weight_grads_mat);
    
    # Start one step before the last weight and bias transformation
    for (my $m_to_n_func_index = $m_to_n_func_infos->length - 2; $m_to_n_func_index >= 0; $m_to_n_func_index--) {
      
      # Activated Output Change / Output Change
      my $outputs = (R::NDArray::Float)$outputs_in_m_to_n_funcs->get($m_to_n_func_index);

      # Small change in loss function / Small change in bias of this layer (calculated using chain rate of derivative)
      # Multiply the transpose of the weight gradients and bias gradients of the next layer, and multiply each element by the derivative of the activation function.
      my $forword_m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index + 1);
      my $forword_weights_mat = (R::NDArray::Float)$forword_m_to_n_func_info->get("weights_mat");
      my $forword_weights_mat_transpose = MFOP->t($forword_weights_mat);
      my $forword_biase_grads = (R::NDArray::Float)$biase_grads_in_m_to_n_funcs->get($m_to_n_func_index + 1);
      my $forword_biase_grads_mat = MFOP->matrix($forword_biase_grads, $forword_biase_grads->length, 1);
      my $mul_forword_weights_transpose_mat_forword_biase_grads_mat = MFOP->mul($forword_weights_mat_transpose, $forword_biase_grads_mat);
      $mul_forword_weights_transpose_mat_forword_biase_grads_mat->drop_dim;
      my $grads_outputs_to_array_sigmoid = &array_sigmoid_derivative($outputs);
      my $biase_grads = FOP->mul($mul_forword_weights_transpose_mat_forword_biase_grads_mat, $grads_outputs_to_array_sigmoid);

      $biase_grads_in_m_to_n_funcs->set($m_to_n_func_index => $biase_grads);
      
      # Small change in loss function / Small change in weights of this layer (calculated using derivative chain rate)
      my $biase_grads_mat = MFOP->matrix($biase_grads, $biase_grads->length, 1);
      my $inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($m_to_n_func_index);
      
      my $inputs_mat_transpose = MFOP->matrix($inputs, 1, $inputs->length);
      
      my $weights_grads_mat = MFOP->mul($biase_grads_mat, $inputs_mat_transpose);
      
      $weight_grads_mat_in_m_to_n_funcs->set($m_to_n_func_index => $weights_grads_mat);
    }

    my $m_to_n_func_grad_infos = Hash->new({});
    $m_to_n_func_grad_infos->set(biases => $biase_grads_in_m_to_n_funcs);
    $m_to_n_func_grad_infos->set(weights_mat => $weight_grads_mat_in_m_to_n_funcs);
    
    return $m_to_n_func_grad_infos;
  }
  
  static method convert_ubyte_array_to_float_array : float[] ($ubytes : byte[]) {
    my $floats = new float[scalar @$ubytes];
    
    for (my $i = 0; $i < @$ubytes; $i++) {
      $floats->[$i] = ((int)$ubytes->[$i]) & 0xFF;
    }
    
    return $floats;
  }
  
  # Update parameters taking into account learning rate and mini-batch size
  static method update_params : void ($params_ndarray : R::NDArray::Float, $param_grads_ndarray : R::NDArray::Float, $learning_rate : float, $mini_batch_size : int) {
    
    my $params = $params_ndarray->data;
    
    my $param_grads = $param_grads_ndarray->data;
    
    for (my $param_index = 0; $param_index < @$params; $param_index++) {
      my $update_value = ($learning_rate / $mini_batch_size) * $param_grads->[$param_index];
      $params->[$param_index] -= $update_value;
    }
  }
  
  # Finds the index of the maximum value in an array. In the case of ties, returns the first one.
  static method max_index : R::NDArray::Int ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $max = $data->[0];
    my $max_index = 0;
    for (my $i = 0; $i < @$data; $i++) {
      if ($data->[$i] > $max) {
        $max_index = $i;
        $max = $data->[$i];
      }
    }
    
    my $ndarray_ret = IOP->c($max_index);
    
    return $ndarray_ret;
  }
  
  # Generate a probability distribution for the expected output
  static method probabilize_desired_outputs : R::NDArray::Float ($label_number : int) {
    
    my $ndarray_ret = FOP->c(undef, [10]);
    
    my $desired_outputs = $ndarray_ret->data;
    for (my $desired_outputs_index = 0; $desired_outputs_index < 10; $desired_outputs_index++) {
      if ($label_number == $desired_outputs_index) {
        $desired_outputs->[$desired_outputs_index] = 1;
      }
      else {
        $desired_outputs->[$desired_outputs_index] = 0;
      }
    }
    
    return $ndarray_ret;
  }
  
  static method create_xavier_init_value : float ($inputs_length : int) {
      
    return &randn(0, Math->sqrtf((float)1 / $inputs_length));
  }
  
  static method array_create_xavier_init_value : R::NDArray::Float ($array_length : int, $inputs_length : int) {
    
    my $ndarray_ret = FOP->c(undef, [$array_length]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < $array_length; $i++) {
      $data_ret->[$i] = &create_xavier_init_value($inputs_length);
    }
    
    return $ndarray_ret;
  }
  
  static method sigmoid : float ($x : float) {
    
    my $sigmoid = 1.0f / (1.0f + Math->expf(-$x));
    
    return $sigmoid;
  }
  
  static method sigmoid_derivative : float ($x : float) {
    
    my $sigmoid_derivative = &sigmoid($x) * (1 - &sigmoid($x));
    
    return $sigmoid_derivative;
  }
  
  static method array_sigmoid : R::NDArray::Float ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data; $i++) {
      $data_ret->[$i] = &sigmoid($data->[$i]);
    }
    
    return $ndarray_ret;
  }
  
  static method array_sigmoid_derivative : R::NDArray::Float ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data; $i++) {
      $data_ret->[$i] = &sigmoid_derivative($data->[$i]);
    }
    
    return $ndarray_ret;
  }
  
  static method cross_entropy_cost : R::NDArray::Float ($ndarray_x : R::NDArray::Float, $ndarray_y : R::NDArray::Float) {
    
    my $data_x = $ndarray_x->data;
    
    my $data_y = $ndarray_y->data;
    
    my $data_ret = 0f;
    for (my $i = 0; $i < @$data_x; $i++) {
      my $tmp = -$data_y->[$i] * Math->logf($data_x->[$i]) - (1 - $data_y->[$i]) * Math->logf(1 - $data_x->[$i]);
      $data_ret += $tmp;
    }
    
    my $ndarray_ret = FOP->c($data_ret);
    
    return $ndarray_ret;
  }
  
  static method cross_entropy_cost_derivative : R::NDArray::Float ($ndarray_x : R::NDArray::Float, $ndarray_y : R::NDArray::Float) {
    
    my $data_x = $ndarray_x->data;
    
    my $data_y = $ndarray_y->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data_x]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data_x; $i++) {
      $data_ret->[$i] = $data_x->[$i] - $data_y->[$i];
    }
    
    return $ndarray_ret;
  }
  
  # Get rundom value
  # $m : average, $sigma : standard deviation
  static method randn : float ($m : float, $sigma : float) {
    my $seed = $SEED;
    my $r1 = ((double)Fn->crand(\$seed) + 1) / ((double)Fn->RAND_MAX + 2);
    my $r2 = ((double)Fn->crand(\$seed) + 1) / ((double)Fn->RAND_MAX + 2);
    $SEED = $seed;
    
    my $randn = ($sigma * Math->sqrt(-2 * Math->log($r1)) * Math->sin(2 * 3.14159265359 * $r2)) + $m;
    
    return (float)$randn;
  }
  
}
