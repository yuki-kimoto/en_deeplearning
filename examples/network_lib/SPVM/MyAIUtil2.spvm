class MyAIUtil2 : precompile {
  use Fn;
  use Math;
  use List;
  use Hash;
  use IntList;
  use Array;
  use Format;
  use Sys;
  use R::OP::Int as IOP;
  use R::OP::Float as FOP;
  use R::OP::Matrix::Float as MFOP;
  use R::NDArray::Float;
  
  # 総実行回数
  our $TOTAL_COUNT : int;

  # 正解数
  our $ANSWER_MATCH_COUNT : int;
  
  # seed
  our $SEED : int;
  
  INIT {
    $SEED = (int)Sys->time;
  }
  
  
  # シャッフルする
  static method shufflei : int[] ($array_original : int[]) {
    my $size = @$array_original;
    my $array = Array->copy_int($array_original);
    for (my $i = 0; $i < $size; $i++) {
      my $seed = $SEED;
      my $j = Fn->crand(\$seed) % $size;
      $SEED = $seed;
      my $t = $array->[$i];
      $array->[$i] = $array->[$j];
      $array->[$j] = $t;
    }
    return $array;
  }
  
  static method train_deep_network : void ($mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash,
    $epoch_count : int, $mini_batch_size : int, $neurons_count_in_layers : int[], $learning_rate : int)
  {
    # 各層のm個の入力をn個の出力に変換する関数の情報。入力数、出力数、バイアス、重み
    my $m_to_n_func_infos = MyAIUtil2->init_m_to_n_func_infos($neurons_count_in_layers);

    # 訓練データのインデックス(最初の4万枚だけを訓練用データとして利用する。残りの1万枚は検証用データとする)
    my $training_data_indexes = new int[40000];
    my $cur_training_data_index = 0;
    for (my $i = 0; $i < @$training_data_indexes; $i++) {
      $training_data_indexes->[$i] = $i;
    }

    # ミニバッチ単位における各変換関数の情報
    my $m_to_n_func_mini_batch_infos = MyAIUtil2->init_m_to_n_func_mini_batch_infos($m_to_n_func_infos);

    # エポックの回数だけ訓練セットを実行
    for (my $epoch_index = 0; $epoch_index < $epoch_count; $epoch_index++) {
      
      # 訓練データのインデックスをシャッフル(ランダムに学習させた方が汎用化するらしい)
      my $training_data_indexes_shuffle = MyAIUtil2->shufflei($training_data_indexes);
      
      # 確率的勾配降下法を使ってパラメーターを更新
      MyAIUtil2->update_params_sgd(
        $m_to_n_func_mini_batch_infos,
        $m_to_n_func_infos,
        $training_data_indexes_shuffle,
        $mini_batch_size,
        $mnist_train_image_info_spvm,
        $mnist_train_label_info_spvm,
        $learning_rate
      );
    }
  }
  
  static method update_params_sgd : void ($m_to_n_func_mini_batch_infos : List, $m_to_n_func_infos : List, $training_data_indexes_shuffle : int[],
    $mini_batch_size: int, $mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash, $learning_rate : float) {
 
    for (my $training_data_indexes_shuffle_index = 0; $training_data_indexes_shuffle_index < @$training_data_indexes_shuffle; $training_data_indexes_shuffle_index += $mini_batch_size) {
      
      # ミニバッチにおける各変換関数のバイアスの傾きの合計とミニバッチにおける各変換関数の重みの傾きの合計を0で初期化
      for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_mini_batch_infos->length; $m_to_n_func_index++) {
        # ミニバッチにおける各変換関数のバイアスの傾きの合計を0で初期化して作成
        my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
        
        my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
        $m_to_n_func_mini_batch_info->set(biase_grad_totals => FOP->c(undef, [$biase_grad_totals->length]));
        
        # ミニバッチにおける各変換関数の重みの傾きの合計を0で初期化して作成
        my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
        
        # $weight_grad_totals_mat->set_data(FOP->c(undef, [$weight_grad_totals_mat->length]);
        
        Fn->memcpy($weight_grad_totals_mat->data, 0, FOP->c(undef, [$weight_grad_totals_mat->length])->data, 0, 4 * $weight_grad_totals_mat->length);
      }
      
      for (my $mini_batch_index = 0; $mini_batch_index < $mini_batch_size; $mini_batch_index++) {        
        my $training_data_index = $training_data_indexes_shuffle->[$training_data_indexes_shuffle_index + $mini_batch_index];
        
        # バックプロパゲーションを使って重みとバイアスの損失関数に関する傾きを取得
        my $m_to_n_func_grad_infos = MyAIUtil2->backprop($m_to_n_func_infos, $mnist_train_image_info_spvm, $mnist_train_label_info_spvm, $training_data_index);
        
        # バイアスの損失関数に関する傾き
        my $biase_grads_list = (List)$m_to_n_func_grad_infos->get("biases");
        
        # 重みの損失関数に関する傾き
        my $weight_grads_mat_list = (List)$m_to_n_func_grad_infos->get("weights_mat");

        # ミニバッチにおける各変換関数のバイアスの傾きの合計とミニバッチにおける各変換関数の重みの傾きを加算
        for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_mini_batch_infos->length; $m_to_n_func_index++) {
          my $m_to_n_func_info = $m_to_n_func_infos->get($m_to_n_func_index);
          
          # ミニバッチにおける各変換関数のバイアスの傾きを加算
          my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
          my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
          my $biase_grads = (R::NDArray::Float)$biase_grads_list->get($m_to_n_func_index);
          $biase_grad_totals = FOP->add($biase_grad_totals, $biase_grads);

          # ミニバッチにおける各変換関数の重みの傾きを加算
          my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
          my $weight_grads_mat = (R::NDArray::Float)$weight_grads_mat_list->get($m_to_n_func_index);
          
          # $weight_grad_totals_mat->{data} = FOP->add($weight_grad_totals_mat, $weight_grads_mat)->data;
          
          Fn->memcpy($weight_grad_totals_mat->data, 0, FOP->add($weight_grad_totals_mat, $weight_grads_mat)->data, 0, 4 * @{$weight_grad_totals_mat->data});
        }
      }

      # 各変換関数のバイアスと重みをミニバッチの傾きの合計を使って更新
      for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
        
        # 各変換関数のバイアスを更新(学習率を考慮し、ミニバッチ数で割る)
        my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
        my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
        
        my $m_to_n_func_mini_batch_info = (Hash)$m_to_n_func_mini_batch_infos->get($m_to_n_func_index);
        my $biase_grad_totals = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("biase_grad_totals");
        MyAIUtil2->update_params($biases, FOP->c($biase_grad_totals), $learning_rate, $mini_batch_size);
        
        # 各変換関数の重みを更新(学習率を考慮し、傾きの合計をミニバッチ数で、ミニバッチ数で割る)
        my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
        my $weight_grad_totals_mat = (R::NDArray::Float)$m_to_n_func_mini_batch_info->get("weight_grad_totals_mat");
        MyAIUtil2->update_params($weights_mat, $weight_grad_totals_mat, $learning_rate, $mini_batch_size);
      }
    }
  }

  static method init_m_to_n_func_infos : List ($neurons_count_in_layers : int[]) {
    # 各層のm個の入力をn個の出力に変換する関数の情報。入力数、出力数、バイアス、重み
    my $m_to_n_func_infos = List->new([]);
    
    # 各層のニューロン数からmからnへの変換関数の情報を作成
    for (my $i = 0; $i < @$neurons_count_in_layers - 1; $i++) {
      my $inputs_length = $neurons_count_in_layers->[$i];
      my $outputs_length = $neurons_count_in_layers->[$i + 1];
      
      # バイアスを0で初期化
      my $biases = FOP->c(undef, [$outputs_length]);
      
      # Xivierの初期値で重みを初期化。重みは列優先行列
      my $weights_length = $outputs_length * $inputs_length;
      my $weights_mat = MFOP->matrix(MyAIUtil2->array_create_xavier_init_value($weights_length, $inputs_length), $outputs_length, $inputs_length);
      
      # 変換関数の情報を設定
      my $m_to_n_func_info = Hash->new({});
      $m_to_n_func_info->set_int(inputs_length => $inputs_length);
      $m_to_n_func_info->set_int(outputs_length => $outputs_length);
      $m_to_n_func_info->set(biases => $biases);
      $m_to_n_func_info->set(weights_mat => $weights_mat);
      
      $m_to_n_func_infos->push($m_to_n_func_info);
    }
    
    return $m_to_n_func_infos;
  }

  static method init_m_to_n_func_mini_batch_infos : List ($m_to_n_func_infos : List) {
    # ミニバッチ単位における各変換関数の情報
    
    my $m_to_n_func_mini_batch_infos = List->new_len(new object[0], $m_to_n_func_infos->length);

    # ミニバッチにおける各変換関数のバイアスの傾きの合計とミニバッチにおける各変換関数の重みの傾きの合計を0で初期化して作成
    # メモリ領域を繰り返しつかうためここで初期化
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
      my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
      
      # バイアスの長さ
      my $biases_length = $biases->length;

      # ミニバッチにおける各変換関数のバイアスの傾きの合計を0で初期化して作成
      my $biase_grad_totals = FOP->c(undef, [$biases_length]);

      # ミニバッチにおける各変換関数の重みの傾きの合計を0で初期化して作成
      my $weight_grad_totals_mat = MFOP->matrix(undef, $weights_mat->nrow, $weights_mat->ncol);
      
      my $m_to_n_func_mini_batch_info = Hash->new({});
      $m_to_n_func_mini_batch_info->set(biase_grad_totals => $biase_grad_totals);
      $m_to_n_func_mini_batch_info->set(weight_grad_totals_mat => $weight_grad_totals_mat);
      
      $m_to_n_func_mini_batch_infos->set($m_to_n_func_index => $m_to_n_func_mini_batch_info);
    }
    
    return $m_to_n_func_mini_batch_infos;
  }

  # バックプロパゲーション
  static method backprop : Hash ($m_to_n_func_infos : List, $mnist_train_image_info_spvm : Hash, $mnist_train_label_info_spvm : Hash, $training_data_index : int)  {
    
    my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get(0);
    my $first_inputs_length = (int)$m_to_n_func_info->get("inputs_length");
    
    # 入力(0～255の値を0～1に変換)
    my $mnist_train_image_rows_count = (int)$mnist_train_image_info_spvm->get("rows_count");
    my $mnist_train_image_columns_count = (int)$mnist_train_image_info_spvm->get("columns_count");
    my $image_unit_length = $mnist_train_image_rows_count * $mnist_train_image_columns_count;
    my $mnist_train_image_data = (byte[])$mnist_train_image_info_spvm->get("data");
    
    my $first_inputs_raw_uint8 = Array->copy_byte($mnist_train_image_data, $image_unit_length * $training_data_index, $image_unit_length);
    my $first_inputs_raw_float = FOP->c(MyAIUtil2->convert_ubyte_array_to_float_array($first_inputs_raw_uint8))->data;
    
    my $first_inputs = FOP->scadiv(FOP->c($first_inputs_raw_float), FOP->c(255f));
    
    # 期待される出力を確率分布化する
    my $label_numbers = (IntList)$mnist_train_label_info_spvm->get("label_numbers");
    my $label_number = (Int)$label_numbers->get($training_data_index);
    my $desired_outputs = MyAIUtil2->probabilize_desired_outputs($label_number)->data;
    
    # 各変換関数のバイアスの傾き
    my $biase_grads_in_m_to_n_funcs = List->new_len(new R::NDArray::Float[0], $m_to_n_func_infos->length);
    
    # 各変換関数の重みの傾き
    my $weight_grads_mat_in_m_to_n_funcs = List->new_len(new R::NDArray::Float[0], $m_to_n_func_infos->length);
    
    # バイアスの傾きと重みの傾きの初期化
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      my $inputs_length = (int)$m_to_n_func_info->get("inputs_length");
      my $outputs_length = (int)$m_to_n_func_info->get("outputs_length");
      
      # バイアスの傾きを0で初期化
      $biase_grads_in_m_to_n_funcs->set($m_to_n_func_index => FOP->c(undef, [$outputs_length]));

      # 重みの傾きを0で初期化
      $weight_grads_mat_in_m_to_n_funcs->set($m_to_n_func_index => MFOP->matrix(undef, $outputs_length, $inputs_length));
    }

    # 各層の入力
    my $inputs_in_m_to_n_funcs = List->new(new R::NDArray::Float[0]);
    $inputs_in_m_to_n_funcs->push($first_inputs);
    
    
    # 各層の活性化された出力
    my $outputs_in_m_to_n_funcs = List->new(new R::NDArray::Float[0]);
    
    # 入力層の入力から出力層の出力を求める
    # バックプロパゲーションのために各層の出力と活性化された出力を保存
    for (my $m_to_n_func_index = 0; $m_to_n_func_index < $m_to_n_func_infos->length; $m_to_n_func_index++) {
      my $cur_inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($inputs_in_m_to_n_funcs->length - 1);
      my $m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index);
      
      # 重み行列
      my $weights_mat = (R::NDArray::Float)$m_to_n_func_info->get("weights_mat");
      
      # 入力行列
      my $cur_inputs_nrow = $cur_inputs->length;
      my $cur_inputs_ncol = 1;
      
      my $cur_inputs_mat = MFOP->matrix($cur_inputs, $cur_inputs_nrow, $cur_inputs_ncol);
      
      # 重みと入力の行列積
      my $mul_weights_inputs_mat = MFOP->mul($weights_mat, $cur_inputs_mat);
      my $mul_weights_inputs = $mul_weights_inputs_mat->data;

      # バイアス
      my $biases = (R::NDArray::Float)$m_to_n_func_info->get("biases");
      
      # 出力 - 重みと入力の行列積とバイアスの和
      my $outputs = FOP->add(FOP->c($mul_weights_inputs), $biases);
      
      # 活性化された出力 - 出力に活性化関数を適用
      my $activate_outputs = MyAIUtil2->array_sigmoid(FOP->c($outputs));
      
      # バックプロパゲーションのために出力を保存
      $outputs_in_m_to_n_funcs->push($outputs);
      
      # バックプロパゲーションのために次の入力を保存
      $inputs_in_m_to_n_funcs->push($activate_outputs);
    }
    
    # 最後の出力
    my $last_outputs = (R::NDArray::Float)$outputs_in_m_to_n_funcs->get($outputs_in_m_to_n_funcs->length - 1);
    
    # 最後の活性化された出力
    my $last_activate_outputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->pop;
    
    # 誤差
    
    my $cost = MyAIUtil2->cross_entropy_cost(FOP->c($last_activate_outputs), FOP->c($desired_outputs))->data->[0];
    print "Cost: " . Format->sprintf("%.3f", [(object)$cost]) . "\n";
    
    # 正解したかどうか
    my $answer = MyAIUtil2->max_index(FOP->c($last_activate_outputs))->data->[0];
    my $desired_answer = MyAIUtil2->max_index(FOP->c($desired_outputs))->data->[0];
    $TOTAL_COUNT++;
    if ($answer == $desired_answer) {
      $ANSWER_MATCH_COUNT++;
    }
    
    # 正解率を出力
    my $match_rate = (double)$ANSWER_MATCH_COUNT / $TOTAL_COUNT;
    print "Match Rate: " . Format->sprintf("%.02f", [(object)(100 * $match_rate)]) . "%\n";
    
    # 活性化された出力の微小変化 / 最後の出力の微小変化 
    my $grad_last_outputs_to_activate_func = MyAIUtil2->array_sigmoid_derivative(FOP->c($last_outputs))->data;
    
    # 損失関数の微小変化 / 最後に活性化された出力の微小変化
    my $grad_last_activate_outputs_to_cost_func = MyAIUtil2->cross_entropy_cost_derivative(FOP->c($last_activate_outputs), FOP->c($desired_outputs))->data;

    # 損失関数の微小変化 / 最後の出力の微小変化 (合成微分)
    my $grad_last_outputs_to_cost_func = FOP->mul(FOP->c($grad_last_outputs_to_activate_func), FOP->c($grad_last_activate_outputs_to_cost_func));

    # 損失関数の微小変化 / 最終の層のバイアスの微小変化
    my $last_biase_grads = $grad_last_outputs_to_cost_func;
    

    # 損失関数の微小変化 / 最終の層の重みの微小変化
    my $last_biase_grads_mat = MFOP->matrix($last_biase_grads, $last_biase_grads->length, 1);
    my $last_inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($inputs_in_m_to_n_funcs->length - 1);
    my $last_inputs_transpose_mat = MFOP->matrix($last_inputs, 1, $last_inputs->length);
    my $last_weight_grads_mat = MFOP->mul($last_biase_grads_mat, $last_inputs_transpose_mat);
      
    $biase_grads_in_m_to_n_funcs->set($biase_grads_in_m_to_n_funcs->length - 1 => $last_biase_grads);
    $weight_grads_mat_in_m_to_n_funcs->set($biase_grads_in_m_to_n_funcs->length - 1 => $last_weight_grads_mat);
    
    # 最後の重みとバイアスの変換より一つ前から始める
    for (my $m_to_n_func_index = $m_to_n_func_infos->length - 2; $m_to_n_func_index >= 0; $m_to_n_func_index--) {
      
      # 活性化された出力の微小変化 / 出力の微小変化
      my $outputs = (R::NDArray::Float)$outputs_in_m_to_n_funcs->get($m_to_n_func_index);

      # 損失関数の微小変化 / この層のバイアスの微小変化(微分の連鎖率を使って求める)
      # 次の層の重みの傾きの転置行列とバイアスの傾きの転置行列をかけて、それぞれの要素に、活性化関数の導関数をかける
      my $forword_m_to_n_func_info = (Hash)$m_to_n_func_infos->get($m_to_n_func_index + 1);
      my $forword_weights_mat = (R::NDArray::Float)$forword_m_to_n_func_info->get("weights_mat");
      my $forword_weights_mat_transpose = MFOP->t($forword_weights_mat);
      my $forword_biase_grads = (R::NDArray::Float)$biase_grads_in_m_to_n_funcs->get($m_to_n_func_index + 1);
      my $forword_biase_grads_mat = MFOP->matrix($forword_biase_grads, $forword_biase_grads->length, 1);
      my $mul_forword_weights_transpose_mat_forword_biase_grads_mat = MFOP->mul($forword_weights_mat_transpose, $forword_biase_grads_mat);
      my $mul_forword_weights_transpose_mat_forword_biase_grads_mat_data = $mul_forword_weights_transpose_mat_forword_biase_grads_mat->data;
      my $grads_outputs_to_array_sigmoid = MyAIUtil2->array_sigmoid_derivative(FOP->c($outputs))->data;
      my $biase_grads = FOP->mul(FOP->c($mul_forword_weights_transpose_mat_forword_biase_grads_mat_data), FOP->c($grads_outputs_to_array_sigmoid));

      $biase_grads_in_m_to_n_funcs->set($m_to_n_func_index => $biase_grads);
      
      # 損失関数の微小変化 / この層の重みの微小変化(微分の連鎖率を使って求める)
      my $biase_grads_mat = MFOP->matrix($biase_grads, $biase_grads->length, 1);
      my $inputs = (R::NDArray::Float)$inputs_in_m_to_n_funcs->get($m_to_n_func_index);
      
      my $inputs_mat_transpose = MFOP->matrix($inputs, 1, $inputs->length);
      
      my $weights_grads_mat = MFOP->mul($biase_grads_mat, $inputs_mat_transpose);
      
      $weight_grads_mat_in_m_to_n_funcs->set($m_to_n_func_index => $weights_grads_mat);
    }

    my $m_to_n_func_grad_infos = Hash->new({});
    $m_to_n_func_grad_infos->set(biases => $biase_grads_in_m_to_n_funcs);
    $m_to_n_func_grad_infos->set(weights_mat => $weight_grads_mat_in_m_to_n_funcs);
    
    return $m_to_n_func_grad_infos;
  }
  
  static method convert_ubyte_array_to_float_array : float[] ($ubytes : byte[]) {
    my $floats = new float[scalar @$ubytes];
    
    for (my $i = 0; $i < @$ubytes; $i++) {
      $floats->[$i] = ((int)$ubytes->[$i]) & 0xFF;
    }
    
    return $floats;
  }
  
  # 学習率とミニバッチ数を考慮してパラメーターを更新
  static method update_params : void ($params_ndarray : R::NDArray::Float, $param_grads_ndarray : R::NDArray::Float, $learning_rate : float, $mini_batch_size : int) {
    
    my $params = $params_ndarray->data;
    
    my $param_grads = $param_grads_ndarray->data;
    
    for (my $param_index = 0; $param_index < @$params; $param_index++) {
      my $update_value = ($learning_rate / $mini_batch_size) * $param_grads->[$param_index];
      $params->[$param_index] -= $update_value;
    }
  }
  
  # 配列の中で最大値のインデックスを求める。同じ数の場合は、最初の方を返す
  static method max_index : R::NDArray::Int ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $max = $data->[0];
    my $max_index = 0;
    for (my $i = 0; $i < @$data; $i++) {
      if ($data->[$i] > $max) {
        $max_index = $i;
        $max = $data->[$i];
      }
    }
    
    my $ndarray_ret = IOP->c($max_index);
    
    return $ndarray_ret;
  }
  
  # 期待される出力を確率分布化する
  static method probabilize_desired_outputs : R::NDArray::Float ($label_number : int) {
    
    my $ndarray_ret = FOP->c(undef, [10]);
    
    my $desired_outputs = $ndarray_ret->data;
    for (my $desired_outputs_index = 0; $desired_outputs_index < 10; $desired_outputs_index++) {
      if ($label_number == $desired_outputs_index) {
        $desired_outputs->[$desired_outputs_index] = 1;
      }
      else {
        $desired_outputs->[$desired_outputs_index] = 0;
      }
    }
    
    return $ndarray_ret;
  }
  
  static method create_xavier_init_value : float ($inputs_length : int) {
      
    return &randn(0, Math->sqrtf((float)1 / $inputs_length));
  }
  
  static method array_create_xavier_init_value : R::NDArray::Float ($array_length : int, $inputs_length : int) {
    
    my $ndarray_ret = FOP->c(undef, [$array_length]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < $array_length; $i++) {
      $data_ret->[$i] = &create_xavier_init_value($inputs_length);
    }
    
    return $ndarray_ret;
  }
  
  static method sigmoid : float ($x : float) {
    
    my $sigmoid = 1.0f / (1.0f + Math->expf(-$x));
    
    return $sigmoid;
  }
  
  static method sigmoid_derivative : float ($x : float) {
    
    my $sigmoid_derivative = &sigmoid($x) * (1 - &sigmoid($x));
    
    return $sigmoid_derivative;
  }
  
  static method array_sigmoid : R::NDArray::Float ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data; $i++) {
      $data_ret->[$i] = &sigmoid($data->[$i]);
    }
    
    return $ndarray_ret;
  }
  
  static method array_sigmoid_derivative : R::NDArray::Float ($ndarray : R::NDArray::Float) {
    
    my $data = $ndarray->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data; $i++) {
      $data_ret->[$i] = &sigmoid_derivative($data->[$i]);
    }
    
    return $ndarray_ret;
  }
  
  static method cross_entropy_cost : R::NDArray::Float ($ndarray_x : R::NDArray::Float, $ndarray_y : R::NDArray::Float) {
    
    my $data_x = $ndarray_x->data;
    
    my $data_y = $ndarray_y->data;
    
    my $data_ret = 0f;
    for (my $i = 0; $i < @$data_x; $i++) {
      my $tmp = -$data_y->[$i] * Math->logf($data_x->[$i]) - (1 - $data_y->[$i]) * Math->logf(1 - $data_x->[$i]);
      $data_ret += $tmp;
    }
    
    my $ndarray_ret = FOP->c($data_ret);
    
    return $ndarray_ret;
  }
  
  static method cross_entropy_cost_derivative : R::NDArray::Float ($ndarray_x : R::NDArray::Float, $ndarray_y : R::NDArray::Float) {
    
    my $data_x = $ndarray_x->data;
    
    my $data_y = $ndarray_y->data;
    
    my $ndarray_ret = FOP->c(undef, [scalar @$data_x]);
    
    my $data_ret = $ndarray_ret->data;
    for (my $i = 0; $i < @$data_x; $i++) {
      $data_ret->[$i] = $data_x->[$i] - $data_y->[$i];
    }
    
    return $ndarray_ret;
  }
  
  # Get rundom value
  # $m : average, $sigma : standard deviation
  static method randn : float ($m : float, $sigma : float) {
    my $seed = $SEED;
    my $r1 = ((double)Fn->crand(\$seed) + 1) / ((double)Fn->RAND_MAX + 2);
    my $r2 = ((double)Fn->crand(\$seed) + 1) / ((double)Fn->RAND_MAX + 2);
    $SEED = $seed;
    
    my $randn = ($sigma * Math->sqrt(-2 * Math->log($r1)) * Math->sin(2 * 3.14159265359 * $r2)) + $m;
    
    return (float)$randn;
  }
  
}
