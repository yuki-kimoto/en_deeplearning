<h2>PerlとSPVMで学ぶディープラーニング入門 - 国産AI 日本の期待の星プロジェクト</h2>

PerlとSPVMで学ぶディープラーニング入門です。SPVMは、Perlの処理を高速化できるモジュールです。ディープラーニングでは、計算資源を最適化することが求められますが、Perlは、数値計算能力は、ディープラーニングの必要を満たさないので、SPVMというモジュールを使います。

PerlとSPVMで学ぶディープラーニング入門は、Perlゼミの<a href="http://school.perlzemi.com/">プログラミングスクール</a>の講座のひとつです。PerlでAIの一つの分野であるディープラーニングの基礎を学び、実用的に使えることを目標に作られています。

「国産AI 日本の期待の星プロジェクト」と名づけられました。

<h3>ディープラーニングとは何か?</h3>

ディープラーニングは、機械学習の一つの分野です。

機械学習では、入力から正しい出力を得られるように、パラメーターを自動的に調整します。パラメーターを自動的に調整する仕組みを、プログラムとして実装するので、機械的に学習が進むのです。

たとえば、入力が「人物の写真」だとして、この人は「眼鏡をかけている」「眼鏡をかけていない」というのが出力です。パターン認識と呼ばれる基本的な分野です。

眼鏡をかけているかどうかの正答率を上昇させていくことが、機械学習の目標です。

ディープラーニングは、機械学習の中でも、人間の神経伝達の仕組みを模倣した、ニューラルネットワークを利用した機械学習です。

ディープラーニングは、ニューラルネットワークの中で、深層学習と呼ばれ、複数の入力と複数の出力を、多段階に連続的につなぎ、最終的な出力を得る方法です。

<pre>
人物の写真
↓
(x0, x1, x2, x3)
↓
(y0, y1, y2, y3, y4)
↓
(z0, z1, z2)
↓
(眼鏡をかけている、眼鏡をかけていない)
</pre>

実際は、x1～xnまでは、もっと長いです。y1～ykまでももっと長いです。xの個数とyの個数は異なっても構いませんし、一般的には、異なるようにします。

また、段数は、この例では、x, y, zですが、これも、もっと長くなります。

入力から、複数入力と複数出力を多段につなげていって、出力を得るというところがポイントです。

googleが作ったような、100億個のニューロンに該当するものを作るとすれば、各層は1億個程度、層の数は100層くらいになります。ひえー。

<h3>数学の言葉をできるだけ使わないで解説</h3>

Perlで学ぶディープラーニング入門では、数学の言葉をできるだけ使わないで解説します。

まずプログラミングのサンプルで、入力と出力を提示します。そのあとで、補助的に高校生の数学で理解できる範囲で説明します。

「えっ、そんなことができるの?」

でも、よく考えてください。

「中身、全部ソースコードじゃないですか!!!」。

「数学はわからなくっても、コードは読めるはず!!!」

やってみてからのお楽しみ。

<h3>ライブラリ使ったらいいんじゃないの?</h3>

僕は、ディープラーニングを実現するための本質は、大量のデータ量をWebから取得することと、電気代を減らせることにかかっていると考えています。

そもそも、この2本がなければ、ディープラーニングは、底に穴が開いているようなものです。

特定のアプリケーション用にディープラーニングを最適化するために、ディープラーニングの仕組みそのものを理解して、プログラミングが書ける必要があると考えています。

汎用的なライブラリは、計算資源の利用効率を最適化してくれないのです。

電気代を、クラウドコンピューター事業者に吸い込まれて「はい。おしまい。」それじゃ、悲しいですよね。

<h3>ディープラーニング入門</h3>

<ul>
  <li><a href="/blog/20200302113052.html">中間層における計算 - m個の入力をn個の出力に変換する</a></li>
  <li><a href="/blog/20200306113052.html">出力層における計算</a></li>
  <li><a href="/blog/20200311113241.html">各層のパラメーターの初期値の設定方法</a></li>
</ul>

<h3>Perl+ディープラーニングで手書き文字認識</h3>

<ul>
  <li><a href="/blog/20200908120907.html">Perl+ディープラーニングで手書き文字認識</a></li>
</ul>

<h3>MINISTデータセットの読み込み</h3>

<ul>
  <li><a href="/blog/20200907120907.html">MINIST画像情報を読み込む</a></li>
  <li><a href="/blog/20200909120907.html">MINISTラベル情報を読み込む</a></li>
</ul>

<h3>基礎知識</h3>

<ul>
  <li><a href="/blog/20200905120907.html">隠れ層におけるニューロンの数を表現する</a></li>
</ul>

<h3>訓練データに対する前処理</h3>

<ul>
  <li><a href="/blog/20200906120907.html">訓練データをランダムにシャッフルする</a></li>
</ul>
