<h2>Perlで学ぶディープラーニング入門 - 基礎と実用が学べる国産AIプロダクト</h2>

Perlで学ぶディープラーニング入門です。ディープラーニングのアルゴリズムをPerlで書きます。基礎と実用が学べる国産のAIプロダクトです。

<iframe width="560" height="315" src="https://www.youtube.com/embed/dTKY0kor50A" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

ディープラーニングでは、計算資源を最適化することが求められるので、Perlの数値計算能力は、ディープラーニングの必要を満たさないのではないかという疑問が浮かぶと思います。

これを解決するために、Perlのソースコードの一部をSPVMへ移植し、C言語の関数やcudaのcuBLASにバインディングして、高速化するという手法を書いていきます。SPVMは、C言語へのトランスパイラ機能とC/C++/cudaへのバインディング機能を提供するPerlのモジュールです。

<h3>ディープラーニングとは何か?</h3>

ディープラーニングは、機械学習の一つの分野です。

機械学習では、入力から正しい出力を得られるように、パラメーターを自動的に調整します。パラメーターを自動的に調整する仕組みを、プログラムとして実装するので、機械的に学習が進むのです。

たとえば、入力が「人物の写真」だとして、この人は「眼鏡をかけている」「眼鏡をかけていない」というのが出力です。パターン認識と呼ばれる基本的な分野です。

眼鏡をかけているかどうかの正答率を上昇させていくことが、機械学習の目標です。

ディープラーニングは、機械学習の中でも、人間の神経伝達の仕組みを模倣した、ニューラルネットワークを利用した機械学習です。

ディープラーニングは、ニューラルネットワークの中で、深層学習と呼ばれ、複数の入力と複数の出力を、多段階に連続的につなぎ、最終的な出力を得る方法です。

<pre>
人物の写真
↓
(x0, x1, x2, x3)
↓
(y0, y1, y2, y3, y4)
↓
(z0, z1, z2)
↓
(眼鏡をかけている、眼鏡をかけていない)
</pre>

実際は、x1～xnまでは、もっと長いです。y1～ykまでももっと長いです。xの個数とyの個数は異なっても構いませんし、一般的には、異なるようにします。

また、段数は、この例では、x, y, zですが、これも、もっと長くなります。

入力から、複数入力と複数出力を多段につなげていって、出力を得るというところがポイントです。

100億個のニューロンに該当するものを作るとすれば、各層のニューロン数は1億個程度、層の数は100層くらいになります。

<h3>数学の言葉をできるだけ使わないで解説</h3>

Perlで学ぶディープラーニング入門では、数学の言葉をできるだけ使わないで解説します。

まずプログラミングのサンプルで、入力と出力を提示します。そのあとで、補助的に高校生の数学で理解できる範囲で説明します。

「えっ、そんなことができるの?」

でも、よく考えてください。

「中身、全部ソースコードじゃないですか!!!」。

「数学はわからなくっても、コードは読めるはず!!!」

やってみてからのお楽しみ。

<h3>ライブラリ使ったらいいんじゃないの?</h3>

僕は、ディープラーニングを実現するための本質は、大量のデータをWebや業務データから取得できることと、電気代を減らして処理速度を上げることにかかっていると考えています。

そもそも、この2本がなければ、ディープラーニングは、底に穴が開いているようなものです。

特定のアプリケーション用にディープラーニングを最適化するために、ディープラーニングの仕組みそのものを理解して、プログラミングが書ける必要があると考えています。

汎用的なライブラリは、計算資源の利用効率を最適化してくれないのです。

<h3>基礎知識</h3>

<ul>
  <li><a href="/blog/20200905120907.html">隠れ層におけるニューロンの数を表現する</a></li>
  <li><a href="/blog/20200904120907.html">エポックという単位は訓練データを一巡することを指す</a></li>
  <li><a href="/blog/20200830120907.html">バッチサイズとは - オンライン学習、ミニバッチ学習、バッチ学習</a></li>
  <li><a href="/blog/20200923123308.html">学習率とは</a></li>
  <li><a href="/blog/20200921123308.html">勾配(こうばい)とは</a></li>
</ul>

<h3>入力処理</h3>

<ul>
  <li><a href="/blog/20200907120907.html">MINIST画像情報を読み込む</a></li>
  <li><a href="/blog/20200909120907.html">MINISTラベル情報を読み込む</a></li>
  <li><a href="/blog/20200906120907.html">訓練データをランダムにシャッフルする</a></li>
</ul>

<h3>重みとバイアス</h3>

<ul>
  <li><a href="/blog/20201016143424.html">バイアスとは</a></li>
  <li><a href="/blog/20201015143424.html">重みとは</a></li>
  <li><a href="/blog/20200311113241.html">各層の重みとバイアスの初期値の設定方法</a></li>
  <li><a href="/blog/20201005144439.html">正規分布に従う乱数を求める</a></li>
  <li><a href="/blog/20201007144439.html">Xavierの初期値</a></li>
  <li><a href="/blog/20201006144439.html">Heの初期値</a></li>
</ul>

<h3>ディープラーニングで使う数学</h3>

<h4>入出力</h4>

<ul>
  <li><a href="/blog/20200302113052.html">隠れ層における計算 - m個の入力をn個の出力に変換する</a></li>
  <li><a href="/blog/20200306113052.html">出力層における計算</a></li>
  <li><a href="/blog/20200915121719.html">ディープラーニングで初期入力から最終出力を得る計算過程</a></li>
</ul>

<h4>ベクトル</h4>

<ul>
  <li><a href="/blog/20200913103640.html">ベクトルの和を求める</a></li>
  <li><a href="/blog/20200828120907.html">ベクトルの差を求める</a></li>
  <li><a href="/blog/20200924123308.html">ベクトルの内積の計算</a></li>
</ul>

<h4>行列</h4>

<ul>
  <li><a href="/blog/20200928161518.html">行列とは</a></li>
  <li><a href="/blog/20200929161518.html">列優先行列を作成する</a></li>
  <li><a href="/blog/20200912123308.html">行列の和を求める</a></li>
  <li><a href="/blog/20200829120907.html">行列の差を求める</a></li>
  <li><a href="/blog/20200914103640.html">行列の積の計算</a></li>
  <li><a href="/blog/20200917123308.html">転置行列を求める</a></li>
</ul>

<h4>微分</h4>

<ul>
  <li><a href="/blog/20201020085300.html">傾き(かたむき)とは</a></li>
  <li><a href="/blog/20201021085550.html">多段の関数の場合の傾きの求め方 - 合成関数の微分</a></li>
  <li><a href="/blog/20200919123308.html">導関数とは</a></li>
  <li><a href="/blog/20201026095954.html">逆誤差伝播法 - バックプロパゲーション 書き始め</a></li>
</ul>

<h3>活性化関数</h3>

<ul>
  <li><a href="/blog/20200902120907.html">活性化関数とは</a></li>
  <li><a href="/blog/20200903120907.html">シグモイド関数</a></li>
  <li><a href="/blog/20200920123308.html">シグモイド関数の導関数</a></li>
  <li><a href="/blog/20200911102242.html">ReLU関数</a></li>
  <li><a href="/blog/20201001161518.html">ReLU関数の導関数</a></li>
  <li><a href="/blog/20201019123741.html">tanh関数</a></li>
  <li><a href="/blog/20201018123741.html">tanh関数の導関数</a></li>
</ul>

<h3>出力層</h3>

<ul>
  <li><a href="/blog/20200916101844.html">期待される出力を確率で表現する</a></li>
  <li><a href="/blog/20201002161518.html">softmax関数</a></li>
  <li><a href="/blog/20200927161518.html">softmaxクロスエントロピーの導関数</a></li>
</ul>

<h3>損失関数</h3>

<ul>
  <li><a href="/blog/20200901120907.html">損失関数とは</a></li>
  <li><a href="/blog/20200910120907.html">二乗和誤差を求める</a></li>
  <li><a href="/blog/20200831120907.html">クロスエントロピー誤差</a></li>
</ul>

<h3>重みとバイアスの更新</h3>

<ul>
  <li><a href="/blog/20201023083657.html">パラメーター更新最適化アルゴリズムとは</a></li>
  <li><a href="/blog/20201017123741.html">確率的勾配降下法(SGD) - 重みとバイアスのパラメータの更新</a></li>
  <li><a href="/blog/20201022083657.html">Adam - SGDの改善</a></li>
</ul>

<h3>Perl+ディープラーニングで手書き文字認識</h3>

<ul>
  <li><a href="/blog/20200908120907.html">Perl+ディープラーニングで手書き文字認識</a></li>
  <li><a href="/blog/20200926161518.html">ピュアPerlで書いたMNIST手書き認識ディープラーニング</a></li>
</ul>

<h3>SPVMを使ったディープラーニング高速化</h3>

