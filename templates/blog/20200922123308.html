<h2>バックプロパゲーション - 逆誤差伝播法</h2>

ディープラーニングで最も理解するのが難しいアルゴリズム、バックプロパゲーション(逆誤差伝播法)について解説します。

<h3>バックプロパゲーションとは何か</h3>

バックプロパゲーションは、各層の重みとバイアスのパラメーターに対する損失関数の傾きを求めるアルゴリズムの一つです。

各層の重みとバイアスのパラメーターに対する損失関数の傾きを求めるアルゴリズムには、バックプロパゲーションよりも理解するのにより簡単な方法もあります。

ディープラーニングでは、パフォーマンスが重要なので、サンプルコードで最初から、バックプロパゲーションが採用されていると考えてください。

バックプロパゲーションをいくつかの要素に分けて考えていきましょう。

<h3>最後の層の個々の重みとバイアスのパラメーターに対する損失関数の傾きを求める</h3>

バックプロパゲーションは、後ろから進んでいきます。最終出力は損失関数です。損失関数は、誤差の指標になります。

まず最初に、最後の層のバイアスパラメータと損失関数の関係を考えましょう。重みとバイアスは定義されていると考えてください。vec_addはベクトルの積、mat_mulは行列積、vec_subは、ベクトルの差を求める関数だと考えてください。

<pre>
# 重み
my $weights = [..., ..., ..., ..., ..., ..., ..., ..., ..., ..., ..., ...,];

# バイアス
my $biases = [..., ..., ...];

# 期待される出力
my $desired_outputs = [1, 0, 0];

# 入力
my $inputs = [0.3, 0.2, 0, 0.5];

# 出力
my $outputs = vec_add(mat_mul($wieghts, $inputs), $biases);

# 活性化された出力
my $activate_outputs = activate($outputs);

# 損失関数の結果(誤差の指標)
my $cost = cost(vec_sub($desired_outputs, $activate_outputs);
</pre>

ここで、バイアスの一つの値を動かしたときに、損失関数がどう動くかを見たいと考えてください。たとえば、バイアス1を微小値0.001増加させたときに、損失関数が0.002減少したとします。

この時、バイアス1に対する損失関数の傾きは「-2 = -0.002/0.001」です。そして順番に、すべてのバイアスと、すべての重みについて、求めていきます。バックプロパゲーションは、この値を速く求めるためのアルゴリズムです。

ここからは、難しいことはもう考えません。ソフトウェアエンジニアとしては、こうすれば、それが、求まるという考え方をしましょう。数学的知見のあるAIの研究者が導き出した方法を実装します。

<h4>バイアスに対する損失関数の傾き</h4>

まず、最初に、最初のバイアスに対する損失関数の傾き求めます。

<pre>
# ひとつのバイアスの損失関数に関する傾き
#   = 活性化関数の導関数(ひとつの出力) * 損失関数の導関数(ひとつの期待される出力, ひとつの活性化された出力);
my $biase_grads0 = activate_derivative($outputs->[0]) * cost_derivative($desired_outputs->[0], $activate_outputs->[0]);
</pre>

すべてのバイアスに対する損失関数の傾きを求めるので、forループします。

<pre>
my $biase_grads = [];
for (my $i = 0; $i < @$biases; $i++) {
  $biase_grads->[$i] = activate_derivative($outputs->[0]) * cost_derivative($desired_outputs->[0], $activate_outputs->[0]);
}
</pre>
