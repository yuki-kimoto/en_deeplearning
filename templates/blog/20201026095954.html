<h2>逆誤差伝播法 - バックプロパゲーション 書き始め</h2>

ディープラーニングにおいて、逆誤差伝播法(バックプロパゲーション)は、ソースコードを読むときに、最も理解するのが難しい部分だと思います。

ソースコードの中に、行列を転置する部分がでてきます。微分がでてきます。入力と出力と活性化された出力と重みとバイアスが入り混じっています。最終出力と中間出力の部分では、傾きを求める式の表現が異なっています。

m入力からn出力の変換関数において、微分とはいったい、どのような操作なのだろう。高校数学ではならっていないぞ。ヤコビ行列? 結局、高度な数学の概念を理解していない限り、ディープラーニングは、理解できないのかな?

ソースコードを一目見たときに「いったい何をしているだろう?」と疑問が浮かぶと思います。

これは僕も感じたことです。ただ、ソースコードの解読を進めていったときに、Web上に書いてある情報とはかなりの違いがあることもわかってきました。

ディープラーニングは、「高校数学」と「多段の関数における傾きの求め方」と「for文の繰り返しの理解」があれば、読み解けます。行列の計算は、ソフトウェアにおけるパフォーマンスの高速化と数学的抽象化のために、行われているということがわかってきます。

ひとつの要素なら具体的に読み解ける、それが繰り返されているだけだ、その繰り返しを行列を使うと簡単に速く計算できるんだということです。

今、文系で仕事に関わる方も、AIディープラーニングについて、理解を深めたいという需要を感じています。誤解のあるまま結論を急いだ判断をしないように、AIとは何かをもう少し正しく知りたいという需要です。ただ、高度な数学的理解が、壁になって、理解するのが難しくなっているということも感じます。

Perlで学ぶディープラーニング入門は、if文とfor文と高校数学(傾きの理解)+アルファで、ディープラーニングが理解できるように、工夫されたサイトです。

<h3>微分の定義</h3>

逆誤差伝播法の理解に必要な高校数学の範囲で理解できるような簡単な微分だけ少し理解しておきましょう。

数学における微分の定義は以下です。

<pre>
# 微分の定義 - 以下の式においてΔxを極限まで0に近づける
f'(x) = (f(x + Δx) - f(x)) / ((x + Δx) - x)
</pre>

「入力の微小変化に対する出力の変化の比」が、微分の定義です。直感的には、ある入力の変化に対して、どれくらい敏感に出力が変化するかということを表します。

<h3>定数関数の微分</h3>

定数の微分は「0」になります。

<pre>
# 定数関数
f(x) = 5;

# 定数関数の微分
f'(x) = 0;
</pre>

上記の「=」は数学記号の等価を表現しています。プログラムにおける代入ではないので、注意してください。

定義に従って計算してみましょう。わかりやすいようにPerlプログラムとして書いてみますね。数学の定義だけだと、ソフトウェアエンジニアは、立ち眩みを起こす可能性があります。

定数関数なので、入力は便宜的なものだと考えてください。数学的な定義においては「$x_delta」を、極限まで0に近づけると考えてください。

<pre>
use strict;
use warnings;

my $x = 3;
my $x_delta = 0.0000001;

# 定数関数の微分
my $const_derivative = (const($x + $x_delta) - const($x)) / (($x + $x_delta) - $x);

# 定数関数
sub const {
  return 5;
}

print $const_derivative;
</pre>

「$x」がどのような値であっても、結果は「0」になることを確認してみてください。

つまり、定数関数の微分は以下で表現されます。

<pre>
# 定数関数の微分
sub const_derivative {
  return 0;
}
</pre>

<h3>関数の微分と導関数、微分係数と傾きの関係</h3>

<a href="/blog/20200919123308.html">導関数</a>は「関数の微分」と全く同じ意味です。このサイトでは「ある関数の微分」のことを「導関数」と呼ぶようにしています。そして、導関数とは、ある入力の微小変化に対する出力の変化の比、つまり、傾き(微分係数とも呼ばれる)を求めるための関数として解説しています。

関数の微分(導関数とも呼ばれる)と微分係数(傾きとも呼ばれる)の違いは、関数の微分は、関数であるため一般的な入力を抽象化しているのに対して、微分係数は、特定の入力に対応した実際の値であるということです。

ソフトウェアエンジニアとしては、導関数とは傾きを求めるための関数であって、傾きとは実際に求めた値のことだと、理解してください。

<h3>一次関数の微分</h3>

一次関数の微分は、xの係数になります。一次関数とは「3x + 2」のようなxの次数が1の関数です。

<pre>
# 一次関数
f(x) = 3x + 2;

# 一次関数の微分(xの係数と同じ値になる)
f'(x) = 3;
</pre>

上記の「=」は数学記号の等価を表現しています。プログラムにおける代入ではないので、注意してください。

定義に従って計算してみましょう。わかりやすいようにPerlプログラムとして書いてみますね。数学の定義だけだと、ソフトウェアエンジニアは、立ち眩みを起こす可能性があります。

一次関数なので、入力は便宜的なものだと考えてください。数学的な定義においては「$x_delta」を、極限まで0に近づけると考えてください。

<pre>
use strict;
use warnings;

my $x = 3;
my $x_delta = 0.0000001;

# 一次関数の微分
my $liner_derivative = (liner($x + $x_delta) - liner($x)) / (($x + $x_delta) - $x);

# 一次関数
sub liner {
  my ($x) = @_;
  
  my $y = 3 * $x + 2;
  
  return $y;
}

print $liner_derivative;
</pre>

「$x」がどのような値でも、結果が「3」になることを確認してみてください。今回の場合は、誤差があって「3.00000000444089」となりました。「$x_delta」が極限まで0に近づくと、3になります。

つまり、「3x + 2」という一次関数の微分は以下で表現されます。

<pre>
# 一次関数の微分
sub liner_derivative {
  return 3;
}
</pre>

<h3>活性化関数の微分</h3>

ディープラーニングでは、ニューロンの活性化の度合いを表現するための活性化関数と呼ばれる関数があります。ただし、実際には、ニューロンの活性の度合いを表現しているというよりも、逆誤差伝播法において、ひとつのパラメーターに対する損失関数の傾きを求めるために導入されていると考えた方がよいのかもしれません。

活性化関数の微分つまり、活性化関数の導関数を導出することはここではしません。ソウトウェアエンジニアとして理解しておく必要があることは、導関数が傾きを求めるための関数であるということです。

導関数がソースコードに出現したら「おっ、ここでは、傾きを求めているのだな」と考えてください。

<pre>
# 活性化関数
sub activate {
  my ($input) = @_;
  
  # ...
  
  return $output;
}

# 活性化関数の微分
sub activate_derivative {
  my ($input) = @_;
  
  # ...
  
  return $output;
}
</pre>

<h3>損失関数の微分</h3>

ディープラーニングでは、誤差の指標である損失関数の値を小さくすることを目標にパラメータが調整されていきます。損失関数の微分について考えてみます。

損失関数への入力は複数です。損失関数の出力はひとつです。

損失関数が、これまで見てきた関数と異なるのは、複数の入力に対して、一つの出力を返すことです。

<pre>
# 損失関数
sub cost {
  my ($inputs) = @_;
  
  my $output;
  
  # ...
  
  return $output;
}
</pre>

ここで「おやっ」となると思います。複数の入力に対して、一つの出力を行う関数の微分なんて習ったことないぞと。そもそも、それはなんだい?

結論を先に書くと、損失関数の微分の結果は、複数の入力に対して、複数の出力になります。

<pre>
# 損失関数の微分
sub cost_derivative {
  my ($inputs) = @_;
  
  my $outputs = [];
  
  # ...
  
  return $outputs;
}
</pre>

ただしこれは、そう見えるというだけで、実は、一つの入力に対して、ひとつの出力があるのを、まとめているだけです。cost_derivative_eachというひとつの入力に対して、ひとつの出力を行う関数を使って、損失関数の微分を書き直してみましょう。

<pre>
sub cost_derivative {
  my ($inputs) = @_;
  
  my $outputs = [];
  for (my $i = 0; $i < @$inputs; $i++) {
    $outputs->[$i] = cost_derivative_each($inputs->[$i]);
  }
  # ...
  
  return $outputs;
}

# 損失関数
sub cost_derivative_each {
  my ($input) = @_;
  
  my $output;
  
  # ...
  
  return $output;
}
</pre>

誤解を恐れずにいえば、注目する一つの入力以外の入力は、定数のようなものとして扱われ、損失関数の微分の結果には何ら影響を与えません。

損失関数の一つであるクロースエントロピー関数をみてください。式は複雑ですが、それぞれの入力の結果が、独立していることを感じ取ってもらえればOKです。

<pre>
# クロスエントロピーコストの導関数
sub cross_entropy_cost_derivative {
  my ($vec_a, $vec_y) = @_;
  
  my $vec_out = [];
  for (my $i = 0; $i < @$vec_a; $i++) {
    $vec_out->[$i] = $vec_a->[$i] - $vec_y->[$i];
  }
  
  return $vec_out;
}
</pre>

<h3>偏微分とは何ですか?</h3>

偏微分とは、微分のことです(笑)。注目する入力変数以外を、すべて定数とみなす微分です。
