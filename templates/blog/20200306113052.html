<h2>出力層における計算</h2>

ディープラーニングにおける出力層の計算について解説します。この記事を読む前提として「<a href="/blog/20200302113052.html">中間層における計算 - m個の入力をn個の出力に変換する</a>」を読んでください。

<h3>パターン認識における出力層</h3>

パターン認識における出力層の計算についての解説です。

<h4>パターン認識(分類問題)における期待される最終出力</h4>

パターン認識(分類問題)における出力層の最終の期待される正しい出力は「(1, 0)」「(0, 1)」のようになります。

たとえば、この写真の人は眼鏡をかけているかどうかという問題があったとして、「眼鏡をかけている」の期待される正しい最終出力は「(1, 0)」、「眼鏡をかけていない」の期待される正しい最終出力は、「(0, 1)」とします。

学習途中は「(0.3 1.2)」のような出力結果が得られますが、入力が眼鏡をかけていた場合に、「(1, 0)」に近づくように、眼鏡をかけていなかった場合に「(0, 1)」に近づくように、パラメーターを調整していくイメージです。パラメーターを自動調整するようなアルゴリズムを書くということが、すなわち学習するという意味です。

犬、猫、豚のように2種類以上の分類でもかまいません。犬であるという正しい出力結果は「(1, 0, 0)」、猫であるという正しい出力結果は「(0, 1, 0」」、豚であるという正しい出力結果は「(0, 0, 1)」であると考えます。

<h4>ReLU関数の出力をそのまま利用</h4>

このような最終出力を計算するために、特別な出力用の関数が必要だと思われがちですが、パターン認識の場合は、ReLU関数の出力をそのまま利用できます。特別に何かをする必要はありません。

Relu関数の出力結果は、活性化しない場合は0、活性化している場合は、その度合いを示す、0より大きく、1を少し超えるような値だったことを思い出してください。

<pre>
(1.99955473915037, 0.979640425704874, 0)
</pre>

上記は学習を続けることで

<pre>
(1, 0, 0)
</pre>

に近づけていくことが可能です。
